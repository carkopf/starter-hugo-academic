---
title: Hallucination, justification, and the role of generative AI in science

event: Polygraphs Conference 
event_url: https://polygraphs.sites.northeastern.edu/events/

location: London

summary: Generative AI systems are disposed to 'hallucinate,' or  fabricate incorrect answers. But they are also used for a variety of scientific modeling tasks. In this talk I investigate how hallucination threatens the reliability of scientific inference, and how that threat can be mitigated.  
abstract: ''

# Talk start and end times.
#   End time can optionally be hidden by prefixing the line with `#`.
date: '2024-10-17T14:00:00Z'
# date_end: '2030-06-01T15:00:00Z'
all_day: true

# Schedule page publish date (NOT talk date).
publishDate: '2017-01-01T00:00:00Z'

authors: []
tags: [deep learning, genAI, hallucination]

# Is this a featured talk? (true/false)
featured: true

image:
  caption: 'Image credit: [**Unsplash**](https://unsplash.com/photos/bzdhc5b3Bxs)'
  focal_point: Right

links:
  - icon: twitter
    icon_pack: fab
    name: Follow
    url: https://twitter.com/CRathkopf
url_code: ''
url_pdf: ''
url_slides: ''
url_video: ''
share: false
profile: false

# Markdown Slides (optional).
#   Associate this talk with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides: example

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects:
  - example
---




