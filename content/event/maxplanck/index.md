---
title: Knowledge transfer from machine learning to neuroscience

event: Cognition Academy
event_url: https://cognition.maxplanckschools.org/en

location: Berlin

summary: An invited talk for the Max Planck School of Cognition  
abstract: 'An increasingly popular methodological view in systems neuroscience says that the best way to understand large-scale neural systems is to (i) define a set of goal-directed behaviors for which that system is responsible, (ii) train an artificial neural network to perform that behavior, (iii) study how the neural network generates that behavior, and (iv) use that knowledge to make inferences about how the biological network does it. Call this ML-neuroscience. As with any novel methodological doctrine, ML-neuroscience has attracted controversy. Skeptics say that biological networks are so different from artificial networks that such comparisons are likely to be misleading. One response to this skepticism is to say that, insofar as we are interested in information-processing properties, artificial neural networks really do exemplify the crucial properties of biological networks. Here, I want to offer a different response that concedes more to the skeptic, but nevertheless manages to defend ML-neuroscience. My strategy is to conceptualize the transfer of knowledge from machine learning models to neurobiological systems as an instance of the more general phenomenon of trans-domain modeling. The history of science is full of cases in which mathematical models developed in one discipline get redeployed in other disciplines, despite the lack of readily observable empirical similarities between the respective target systems. What makes such trans-domain modeling possible? Usually, it is not that the two target systems turn out to be two instances of the same natural kind. If that were the case, we should expect to develop a new body of theory that extends to both systems, and a set of theoretical terms that refer to elements in both. This expectation of theoretical unity is sometimes encouraged by defenders of ML-neuroscience, but ought not be. Trans-domain modeling is often possible because the two systems share rather abstract structural properties that are hard to notice without the use of mathematics. Capturing these abstract structural properties often spurs scientific progress in the absence of theoretical unity. I will illustrate this by means of the well-known Lotka-Volterra model in population biology, which was rediscovered in economic theory. I will then use this case as a guide as I consider which level of abstraction is appropriate for making inferences from machine learning models to neuroscience.'

# Talk start and end times.
#   End time can optionally be hidden by prefixing the line with `#`.
date: '2021-12-01T14:00:00Z'
# date_end: '2030-06-01T15:00:00Z'
all_day: true

# Schedule page publish date (NOT talk date).
publishDate: '2017-01-01T00:00:00Z'

authors: []
tags: [deep learning, functionalism]

# Is this a featured talk? (true/false)
featured: false

image:
  caption: 'Image credit: [**Unsplash**](https://unsplash.com/photos/bzdhc5b3Bxs)'
  focal_point: Right

links:
  - icon: twitter
    icon_pack: fab
    name: Follow
    url: https://twitter.com/CRathkopf
url_code: ''
url_pdf: ''
url_slides: ''
url_video: ''
share: false
profile: false

# Markdown Slides (optional).
#   Associate this talk with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides: example

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects:
  - example
---




