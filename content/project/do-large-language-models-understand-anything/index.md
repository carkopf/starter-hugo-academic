---
title: Do large language models understand?
date: 2022-10-07T17:54:46.497Z
draft: false
featured: false
image:
  filename: featured.png
  focal_point: Smart
  preview_only: false
  caption: Image taken from The Illustrated Transformer.
    https://jalammar.github.io/illustrated-transformer/
profile: false
share: false
---
Large language models have become extremely powerful in the past few years. They can hold conversations, make jokes, and explain the answers to algebra problems. It is hard to avoid the impression that they are *reasoning* about the world. Skeptics say that the performance of large language models depends entirely on statistical associations between words, rather than on an understanding of what they mean. Others are more impressed, and say that large language models understand words in roughly the way that we do. In my view, both positions are exaggerations. The truth is in the middle, but also more stranger, and more difficult to express than either of these. I'm interested in developing better ways of explaining the performance of large language models, both by testing their capacities, and by developing new conceptual resources for describing how they work.

## Associated publications 

{{<cite page="/publication/bigbench" view="3" >}}
